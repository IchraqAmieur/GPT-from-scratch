{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n1. Load our preprocessed data and vocabulary\\n2. Create a custom Dataset class that generates input/target pairs\\n3. Set up a DataLoader for batch processing\\n4. Visualize and verify our data\\n\\nThe key concept here is that for language modeling, we want to predict the next character given the previous characters. For example:\\n- If our text is \"Hello\"\\n- And our block size is 3\\n- One training example would be:\\n  - Input (x): \"Hel\"\\n  - Target (y): \"ell\"\\n'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Creating a Character-Level Dataset for Language Modeling\n",
        "\"\"\"\n",
        "1. Load our preprocessed data and vocabulary\n",
        "2. Create a custom Dataset class that generates input/target pairs\n",
        "3. Set up a DataLoader for batch processing\n",
        "4. Visualize and verify our data\n",
        "\n",
        "The key concept here is that for language modeling, we want to predict the next character given the previous characters. For example:\n",
        "- If our text is \"Hello\"\n",
        "- And our block size is 3\n",
        "- One training example would be:\n",
        "  - Input (x): \"Hel\"\n",
        "  - Target (y): \"ell\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded data shape: torch.Size([147021])\n",
            "Data type: torch.int64\n",
            "\n",
            "Vocabulary size: 77\n",
            "First few characters in itos:\n",
            "0 -> '<PAD>'\n",
            "1 -> '\\n'\n",
            "2 -> '<space>'\n",
            "3 -> '!'\n",
            "4 -> '\"'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "\n",
        "# Step 1: Load our encoded data tensor\n",
        "# This tensor contains our entire text converted to numerical indices\n",
        "encoded_data = torch.load('encoded_script.pt')\n",
        "print(\"Encoded data shape:\", encoded_data.shape)\n",
        "print(\"Data type:\", encoded_data.dtype)\n",
        "\n",
        "# Step 2: Load our vocabulary mappings\n",
        "with open('vocab.json', 'r', encoding='utf-8') as f:\n",
        "    vocab = json.load(f)\n",
        "    \n",
        "# Convert string keys back to integers for itos\n",
        "# (JSON converts all keys to strings, so we need to convert back to integers)\n",
        "itos = {int(k): v for k, v in vocab['itos'].items()}\n",
        "stoi = vocab['stoi']\n",
        "\n",
        "print(\"\\nVocabulary size:\", len(stoi))\n",
        "print(\"First few characters in itos:\")\n",
        "# Print first 5 character mappings as examples\n",
        "for i in range(5):\n",
        "    if itos[i] == '\\n':\n",
        "        char_display = '\\\\n'\n",
        "    elif itos[i] == ' ':\n",
        "        char_display = '<space>'\n",
        "    else:\n",
        "        char_display = itos[i]\n",
        "    print(f\"{i} -> '{char_display}'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input batch shape: torch.Size([4, 128])\n",
            "Target batch shape: torch.Size([4, 128])\n",
            "\n",
            "Sample sequence:\n",
            "Input text: , growing more PALE,\n",
            "then back to Riddle, who is g ...\n",
            "Target text:  growing more PALE,\n",
            "then back to Riddle, who is gr ...\n"
          ]
        }
      ],
      "source": [
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        self.data = data\n",
        "        self.block_size = block_size\n",
        "    \n",
        "    def __len__(self):\n",
        "        # Total possible sequences = data length - block_size\n",
        "        # (since we need block_size+1 characters for each sequence)\n",
        "        return len(self.data) - self.block_size\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Get chunk of data starting at idx with length block_size + 1\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        \n",
        "        # Input is all but last character\n",
        "        x = chunk[:-1]\n",
        "        # Target is all but first character\n",
        "        y = chunk[1:]\n",
        "        \n",
        "        return x, y\n",
        "\n",
        "# Create dataset with block size of 128\n",
        "block_size = 128\n",
        "dataset = CharDataset(encoded_data, block_size)\n",
        "\n",
        "# Create DataLoader with batch size of 4\n",
        "batch_size = 4\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Get a sample batch and print its shape\n",
        "x_batch, y_batch = next(iter(dataloader))\n",
        "print(\"Input batch shape:\", x_batch.shape)\n",
        "print(\"Target batch shape:\", y_batch.shape)\n",
        "\n",
        "# Print a sample sequence and its target\n",
        "print(\"\\nSample sequence:\")\n",
        "seq_idx = 0  # First sequence in batch\n",
        "x_seq = x_batch[seq_idx].tolist()\n",
        "y_seq = y_batch[seq_idx].tolist()\n",
        "\n",
        "print(\"Input text:\", ''.join([itos[i] for i in x_seq[:50]]), \"...\")  # Print first 50 chars\n",
        "print(\"Target text:\", ''.join([itos[i] for i in y_seq[:50]]), \"...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Configuration:\n",
            "Vocabulary Size: 77\n",
            "Block Size: 128\n",
            "Embedding Dimension: 384\n",
            "Number of Layers: 6\n",
            "Number of Attention Heads: 6\n",
            "Dropout: 0.1\n"
          ]
        }
      ],
      "source": [
        "class GPTConfig:\n",
        "    def __init__(self, vocab_size, block_size, embedding_dim=384, num_layers=6, \n",
        "                 num_heads=6, dropout=0.1):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        \n",
        "# Create config\n",
        "config = GPTConfig(\n",
        "    vocab_size=len(stoi),\n",
        "    block_size=block_size,\n",
        "    embedding_dim=384,\n",
        "    num_layers=6,\n",
        "    num_heads=6,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "print(\"Model Configuration:\")\n",
        "print(f\"Vocabulary Size: {config.vocab_size}\")\n",
        "print(f\"Block Size: {config.block_size}\")\n",
        "print(f\"Embedding Dimension: {config.embedding_dim}\")\n",
        "print(f\"Number of Layers: {config.num_layers}\")\n",
        "print(f\"Number of Attention Heads: {config.num_heads}\")\n",
        "print(f\"Dropout: {config.dropout}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = config.embedding_dim\n",
        "        self.head_size = config.embedding_dim // config.num_heads\n",
        "        self.block_size = config.block_size\n",
        "        \n",
        "        # Linear projections\n",
        "        self.key = nn.Linear(self.embedding_dim, self.head_size, bias=False)\n",
        "        self.query = nn.Linear(self.embedding_dim, self.head_size, bias=False)\n",
        "        self.value = nn.Linear(self.embedding_dim, self.head_size, bias=False)\n",
        "        \n",
        "        # Register buffer for attention mask (not a parameter)\n",
        "        mask = torch.triu(torch.ones(self.block_size, self.block_size), diagonal=1).bool()\n",
        "        self.register_buffer('mask', mask)\n",
        "        \n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape  # batch, sequence length, embedding dimension\n",
        "        \n",
        "        # Linear projections\n",
        "        k = self.key(x)    # (B, T, head_size)\n",
        "        q = self.query(x)  # (B, T, head_size)\n",
        "        v = self.value(x)  # (B, T, head_size)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)  # (B, T, T)\n",
        "        wei = wei.masked_fill(self.mask[:T, :T], float('-inf'))   # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1)                              # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        out = wei @ v  # (B, T, head_size)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(config) for _ in range(config.num_heads)])\n",
        "        self.proj = nn.Linear(config.embedding_dim, config.embedding_dim)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Concatenate outputs from all heads\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "        # Project back to embedding dimension\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Expand to 4x embedding dimension\n",
        "            nn.Linear(config.embedding_dim, 4 * config.embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            # Project back to embedding dimension\n",
        "            nn.Linear(4 * config.embedding_dim, config.embedding_dim),\n",
        "            nn.Dropout(config.dropout)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(config)\n",
        "        self.ff = FeedForward(config)\n",
        "        self.ln1 = nn.LayerNorm(config.embedding_dim)\n",
        "        self.ln2 = nn.LayerNorm(config.embedding_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Self-attention with residual connection\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        # Feed-forward with residual connection\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        # Token and position embeddings\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
        "        self.position_embedding = nn.Embedding(config.block_size, config.embedding_dim)\n",
        "        \n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([DecoderBlock(config) for _ in range(config.num_layers)])\n",
        "        \n",
        "        # Final layer norm and projection to vocabulary\n",
        "        self.ln_f = nn.LayerNorm(config.embedding_dim)\n",
        "        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size)\n",
        "        \n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "            \n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        \n",
        "        # Get token and position embeddings\n",
        "        tok_emb = self.token_embedding(idx)  # (B, T, C)\n",
        "        pos_emb = self.position_embedding(torch.arange(T, device=idx.device))  # (T, C)\n",
        "        x = tok_emb + pos_emb  # (B, T, C)\n",
        "        \n",
        "        # Apply transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "            \n",
        "        # Apply final layer norm\n",
        "        x = self.ln_f(x)\n",
        "        \n",
        "        # Project to vocabulary\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "        \n",
        "        # If we have targets, compute loss\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        else:\n",
        "            loss = None\n",
        "            \n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"Generate new tokens after the context in idx.\"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop context to block_size if needed\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            \n",
        "            # Get predictions\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature  # Focus on last time step\n",
        "            \n",
        "            # Optional: top-k sampling\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = float('-inf')\n",
        "            \n",
        "            # Apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            \n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            \n",
        "            # Append to the sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "            \n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of parameters: 10.75M\n",
            "Starting training...\n"
          ]
        }
      ],
      "source": [
        "# Create model and move to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GPT(config).to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
        "\n",
        "# Initialize optimizer\n",
        "learning_rate = 3e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "def train_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (x, y) in enumerate(dataloader):\n",
        "        # Move batch to device\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        logits, loss = model(x, y)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # Gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        \n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        # Print progress\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Batch {batch_idx}: Loss {loss.item():.4f}\")\n",
        "    \n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Training configuration\n",
        "num_epochs = 10\n",
        "save_path = 'gpt_model.pt'\n",
        "best_loss = float('inf')\n",
        "\n",
        "print(\"Starting training...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "Batch 0: Loss 4.4518\n",
            "Batch 100: Loss 2.7120\n",
            "Batch 200: Loss 2.6259\n",
            "Batch 300: Loss 2.6594\n",
            "Batch 400: Loss 2.6229\n",
            "Batch 500: Loss 2.4034\n",
            "Batch 600: Loss 2.3745\n",
            "Batch 700: Loss 2.2929\n",
            "Batch 800: Loss 2.1506\n",
            "Batch 900: Loss 2.2122\n",
            "Batch 1000: Loss 2.1592\n",
            "Batch 1100: Loss 1.9471\n",
            "Batch 1200: Loss 2.1546\n",
            "Batch 1300: Loss 2.0024\n",
            "Batch 1400: Loss 1.9537\n",
            "Batch 1500: Loss 1.8281\n",
            "Batch 1600: Loss 1.9516\n",
            "Batch 1700: Loss 1.8516\n",
            "Batch 1800: Loss 1.9629\n",
            "Batch 1900: Loss 1.6905\n",
            "Batch 2000: Loss 1.8779\n",
            "Batch 2100: Loss 1.8126\n",
            "Batch 2200: Loss 1.8423\n",
            "Batch 2300: Loss 1.6797\n",
            "Batch 2400: Loss 1.7681\n",
            "Batch 2500: Loss 1.6280\n",
            "Batch 2600: Loss 1.8558\n",
            "Batch 2700: Loss 1.4849\n",
            "Batch 2800: Loss 1.5092\n",
            "Batch 2900: Loss 1.6480\n",
            "Batch 3000: Loss 1.5808\n",
            "Batch 3100: Loss 1.8695\n",
            "Batch 3200: Loss 1.6896\n",
            "Batch 3300: Loss 1.5455\n",
            "Batch 3400: Loss 1.6133\n",
            "Batch 3500: Loss 1.7308\n",
            "Batch 3600: Loss 1.4655\n",
            "Batch 3700: Loss 1.5706\n",
            "Batch 3800: Loss 1.4646\n",
            "Batch 3900: Loss 1.7149\n",
            "Batch 4000: Loss 1.5083\n",
            "Batch 4100: Loss 1.5517\n",
            "Batch 4200: Loss 1.4983\n",
            "Batch 4300: Loss 1.5831\n",
            "Batch 4400: Loss 1.6161\n",
            "Batch 4500: Loss 1.5721\n",
            "Batch 4600: Loss 1.5283\n",
            "Batch 4700: Loss 1.3397\n",
            "Batch 4800: Loss 1.5309\n",
            "Batch 4900: Loss 1.5106\n",
            "Batch 5000: Loss 1.4586\n",
            "Batch 5100: Loss 1.4331\n",
            "Batch 5200: Loss 1.2617\n",
            "Batch 5300: Loss 1.3144\n",
            "Batch 5400: Loss 1.4253\n",
            "Batch 5500: Loss 1.4327\n",
            "Batch 5600: Loss 1.4109\n",
            "Batch 5700: Loss 1.4241\n",
            "Batch 5800: Loss 1.4217\n",
            "Batch 5900: Loss 1.3151\n",
            "Batch 6000: Loss 1.3559\n",
            "Batch 6100: Loss 1.4289\n",
            "Batch 6200: Loss 1.4055\n",
            "Batch 6300: Loss 1.1331\n",
            "Batch 6400: Loss 1.1990\n",
            "Batch 6500: Loss 1.1688\n",
            "Batch 6600: Loss 1.3499\n",
            "Batch 6700: Loss 1.3203\n",
            "Batch 6800: Loss 1.1403\n",
            "Batch 6900: Loss 1.1708\n",
            "Batch 7000: Loss 1.1168\n",
            "Batch 7100: Loss 1.1536\n",
            "Batch 7200: Loss 1.1554\n",
            "Batch 7300: Loss 1.1922\n",
            "Batch 7400: Loss 1.0964\n",
            "Batch 7500: Loss 1.0578\n",
            "Batch 7600: Loss 0.9412\n",
            "Batch 7700: Loss 0.8209\n",
            "Batch 7800: Loss 1.2406\n",
            "Batch 7900: Loss 1.0752\n",
            "Batch 8000: Loss 1.0870\n",
            "Batch 8100: Loss 0.9178\n",
            "Batch 8200: Loss 1.1666\n",
            "Batch 8300: Loss 1.1975\n",
            "Batch 8400: Loss 1.1305\n",
            "Batch 8500: Loss 1.0474\n",
            "Batch 8600: Loss 0.8757\n",
            "Batch 8700: Loss 1.1216\n",
            "Batch 8800: Loss 1.0953\n",
            "Batch 8900: Loss 1.1624\n",
            "Batch 9000: Loss 0.8316\n",
            "Batch 9100: Loss 1.0297\n",
            "Batch 9200: Loss 1.1224\n",
            "Batch 9300: Loss 1.1335\n",
            "Batch 9400: Loss 0.9157\n",
            "Batch 9500: Loss 0.9972\n",
            "Batch 9600: Loss 0.8810\n",
            "Batch 9700: Loss 0.8368\n",
            "Batch 9800: Loss 0.9152\n",
            "Batch 9900: Loss 1.1329\n",
            "Batch 10000: Loss 0.9733\n",
            "Batch 10100: Loss 0.8315\n",
            "Batch 10200: Loss 0.9630\n",
            "Batch 10300: Loss 0.7850\n",
            "Batch 10400: Loss 0.9805\n",
            "Batch 10500: Loss 0.9667\n",
            "Batch 10600: Loss 0.9156\n",
            "Batch 10700: Loss 1.0025\n",
            "Batch 10800: Loss 0.7054\n",
            "Batch 10900: Loss 0.8223\n",
            "Batch 11000: Loss 0.7633\n",
            "Batch 11100: Loss 0.7501\n",
            "Batch 11200: Loss 0.9393\n",
            "Batch 11300: Loss 0.7077\n",
            "Batch 11400: Loss 0.9068\n",
            "Batch 11500: Loss 0.9211\n",
            "Batch 11600: Loss 0.9766\n",
            "Batch 11700: Loss 0.6695\n",
            "Batch 11800: Loss 0.7143\n",
            "Batch 11900: Loss 0.7917\n",
            "Batch 12000: Loss 0.9198\n",
            "Batch 12100: Loss 0.5593\n",
            "Batch 12200: Loss 0.9127\n",
            "Batch 12300: Loss 0.8543\n",
            "Batch 12400: Loss 0.9520\n",
            "Batch 12500: Loss 0.7813\n",
            "Batch 12600: Loss 0.9822\n",
            "Batch 12700: Loss 0.8430\n",
            "Batch 12800: Loss 0.6638\n",
            "Batch 12900: Loss 0.7756\n",
            "Batch 13000: Loss 0.7905\n",
            "Batch 13100: Loss 0.7840\n",
            "Batch 13200: Loss 0.7179\n",
            "Batch 13300: Loss 0.6017\n",
            "Batch 13400: Loss 0.7522\n",
            "Batch 13500: Loss 0.6628\n",
            "Batch 13600: Loss 0.5621\n",
            "Batch 13700: Loss 0.8155\n",
            "Batch 13800: Loss 0.7297\n",
            "Batch 13900: Loss 0.7824\n",
            "Batch 14000: Loss 0.5744\n",
            "Batch 14100: Loss 0.6215\n",
            "Batch 14200: Loss 0.6936\n",
            "Batch 14300: Loss 0.7242\n",
            "Batch 14400: Loss 0.5515\n",
            "Batch 14500: Loss 0.6750\n",
            "Batch 14600: Loss 0.5271\n",
            "Batch 14700: Loss 0.6332\n",
            "Batch 14800: Loss 0.5067\n",
            "Batch 14900: Loss 0.6770\n",
            "Batch 15000: Loss 0.7713\n",
            "Batch 15100: Loss 0.5820\n",
            "Batch 15200: Loss 0.6905\n",
            "Batch 15300: Loss 0.5082\n",
            "Batch 15400: Loss 0.5308\n",
            "Batch 15500: Loss 0.5423\n",
            "Batch 15600: Loss 0.5838\n",
            "Batch 15700: Loss 0.4571\n",
            "Batch 15800: Loss 0.3911\n",
            "Batch 15900: Loss 0.5734\n",
            "Batch 16000: Loss 0.7219\n",
            "Batch 16100: Loss 0.3862\n",
            "Batch 16200: Loss 0.5203\n",
            "Batch 16300: Loss 0.4459\n",
            "Batch 16400: Loss 0.6575\n",
            "Batch 16500: Loss 0.5805\n",
            "Batch 16600: Loss 0.4508\n",
            "Batch 16700: Loss 0.4859\n",
            "Batch 16800: Loss 0.5218\n",
            "Batch 16900: Loss 0.5247\n",
            "Batch 17000: Loss 0.5145\n",
            "Batch 17100: Loss 0.4867\n",
            "Batch 17200: Loss 0.5345\n",
            "Batch 17300: Loss 0.5112\n",
            "Batch 17400: Loss 0.4293\n",
            "Batch 17500: Loss 0.4541\n",
            "Batch 17600: Loss 0.3627\n",
            "Batch 17700: Loss 0.4773\n",
            "Batch 17800: Loss 0.4699\n",
            "Batch 17900: Loss 0.4427\n",
            "Batch 18000: Loss 0.4986\n",
            "Batch 18100: Loss 0.3839\n",
            "Batch 18200: Loss 0.4451\n",
            "Batch 18300: Loss 0.5681\n",
            "Batch 18400: Loss 0.5524\n",
            "Batch 18500: Loss 0.3799\n",
            "Batch 18600: Loss 0.4906\n",
            "Batch 18700: Loss 0.4053\n",
            "Batch 18800: Loss 0.5417\n",
            "Batch 18900: Loss 0.4127\n",
            "Batch 19000: Loss 0.4269\n",
            "Batch 19100: Loss 0.4077\n",
            "Batch 19200: Loss 0.4372\n",
            "Batch 19300: Loss 0.4243\n",
            "Batch 19400: Loss 0.3894\n",
            "Batch 19500: Loss 0.4165\n",
            "Batch 19600: Loss 0.5207\n",
            "Batch 19700: Loss 0.4283\n",
            "Batch 19800: Loss 0.4197\n",
            "Batch 19900: Loss 0.3879\n",
            "Batch 20000: Loss 0.4402\n",
            "Batch 20100: Loss 0.3714\n",
            "Batch 20200: Loss 0.4806\n",
            "Batch 20300: Loss 0.3904\n",
            "Batch 20400: Loss 0.4397\n",
            "Batch 20500: Loss 0.4761\n",
            "Batch 20600: Loss 0.4512\n",
            "Batch 20700: Loss 0.5077\n",
            "Batch 20800: Loss 0.4168\n",
            "Batch 20900: Loss 0.3866\n",
            "Batch 21000: Loss 0.3885\n",
            "Batch 21100: Loss 0.3662\n",
            "Batch 21200: Loss 0.3650\n",
            "Batch 21300: Loss 0.4923\n",
            "Batch 21400: Loss 0.4078\n",
            "Batch 21500: Loss 0.3255\n",
            "Batch 21600: Loss 0.3961\n",
            "Batch 21700: Loss 0.3695\n",
            "Batch 21800: Loss 0.4060\n",
            "Batch 21900: Loss 0.3769\n",
            "Batch 22000: Loss 0.4181\n",
            "Batch 22100: Loss 0.3580\n",
            "Batch 22200: Loss 0.3522\n",
            "Batch 22300: Loss 0.3492\n",
            "Batch 22400: Loss 0.4566\n",
            "Batch 22500: Loss 0.2950\n",
            "Batch 22600: Loss 0.3657\n",
            "Batch 22700: Loss 0.4034\n",
            "Batch 22800: Loss 0.3547\n",
            "Batch 22900: Loss 0.3534\n",
            "Batch 23000: Loss 0.3678\n",
            "Batch 23100: Loss 0.3756\n",
            "Batch 23200: Loss 0.3324\n",
            "Batch 23300: Loss 0.3160\n",
            "Batch 23400: Loss 0.3489\n",
            "Batch 23500: Loss 0.3496\n",
            "Batch 23600: Loss 0.3600\n",
            "Batch 23700: Loss 0.3821\n",
            "Batch 23800: Loss 0.3937\n",
            "Batch 23900: Loss 0.3800\n",
            "Batch 24000: Loss 0.4448\n",
            "Batch 24100: Loss 0.3046\n",
            "Batch 24200: Loss 0.3475\n",
            "Batch 24300: Loss 0.3260\n",
            "Batch 24400: Loss 0.3249\n",
            "Batch 24500: Loss 0.4027\n",
            "Batch 24600: Loss 0.2763\n",
            "Batch 24700: Loss 0.3180\n",
            "Batch 24800: Loss 0.2745\n",
            "Batch 24900: Loss 0.2804\n",
            "Batch 25000: Loss 0.4010\n",
            "Batch 25100: Loss 0.3214\n",
            "Batch 25200: Loss 0.2910\n",
            "Batch 25300: Loss 0.3405\n",
            "Batch 25400: Loss 0.2391\n",
            "Batch 25500: Loss 0.3395\n",
            "Batch 25600: Loss 0.3366\n",
            "Batch 25700: Loss 0.3381\n",
            "Batch 25800: Loss 0.3272\n",
            "Batch 25900: Loss 0.3518\n",
            "Batch 26000: Loss 0.3255\n",
            "Batch 26100: Loss 0.3176\n",
            "Batch 26200: Loss 0.3510\n",
            "Batch 26300: Loss 0.2359\n",
            "Batch 26400: Loss 0.3221\n",
            "Batch 26500: Loss 0.3198\n",
            "Batch 26600: Loss 0.3307\n",
            "Batch 26700: Loss 0.2794\n",
            "Batch 26800: Loss 0.4022\n",
            "Batch 26900: Loss 0.3490\n",
            "Batch 27000: Loss 0.3648\n",
            "Batch 27100: Loss 0.3161\n",
            "Batch 27200: Loss 0.3753\n",
            "Batch 27300: Loss 0.3388\n",
            "Batch 27400: Loss 0.3218\n",
            "Batch 27500: Loss 0.3173\n",
            "Batch 27600: Loss 0.2868\n",
            "Batch 27700: Loss 0.3618\n",
            "Batch 27800: Loss 0.2484\n",
            "Batch 27900: Loss 0.3797\n",
            "Batch 28000: Loss 0.3142\n",
            "Batch 28100: Loss 0.3130\n",
            "Batch 28200: Loss 0.3297\n",
            "Batch 28300: Loss 0.3741\n",
            "Batch 28400: Loss 0.3282\n",
            "Batch 28500: Loss 0.2924\n",
            "Batch 28600: Loss 0.3049\n",
            "Batch 28700: Loss 0.3450\n",
            "Batch 28800: Loss 0.3104\n",
            "Batch 28900: Loss 0.3172\n",
            "Batch 29000: Loss 0.3022\n",
            "Batch 29100: Loss 0.2699\n",
            "Batch 29200: Loss 0.2859\n",
            "Batch 29300: Loss 0.3875\n",
            "Batch 29400: Loss 0.3165\n",
            "Batch 29500: Loss 0.2773\n",
            "Batch 29600: Loss 0.3748\n",
            "Batch 29700: Loss 0.2783\n",
            "Batch 29800: Loss 0.3249\n",
            "Batch 29900: Loss 0.3006\n",
            "Batch 30000: Loss 0.2769\n",
            "Batch 30100: Loss 0.2043\n",
            "Batch 30200: Loss 0.2720\n",
            "Batch 30300: Loss 0.3627\n",
            "Batch 30400: Loss 0.3257\n",
            "Batch 30500: Loss 0.2949\n",
            "Batch 30600: Loss 0.3161\n",
            "Batch 30700: Loss 0.2274\n",
            "Batch 30800: Loss 0.4587\n",
            "Batch 30900: Loss 0.2632\n",
            "Batch 31000: Loss 0.2477\n",
            "Batch 31100: Loss 0.2587\n",
            "Batch 31200: Loss 0.2789\n",
            "Batch 31300: Loss 0.2830\n",
            "Batch 31400: Loss 0.2927\n",
            "Batch 31500: Loss 0.2526\n",
            "Batch 31600: Loss 0.3266\n",
            "Batch 31700: Loss 0.2754\n",
            "Batch 31800: Loss 0.2664\n",
            "Batch 31900: Loss 0.3157\n",
            "Batch 32000: Loss 0.2940\n",
            "Batch 32100: Loss 0.2986\n",
            "Batch 32200: Loss 0.2886\n",
            "Batch 32300: Loss 0.2570\n",
            "Batch 32400: Loss 0.3219\n",
            "Batch 32500: Loss 0.2900\n",
            "Batch 32600: Loss 0.2814\n",
            "Batch 32700: Loss 0.2827\n",
            "Batch 32800: Loss 0.2534\n",
            "Batch 32900: Loss 0.2595\n",
            "Batch 33000: Loss 0.2809\n",
            "Batch 33100: Loss 0.3054\n",
            "Batch 33200: Loss 0.2784\n",
            "Batch 33300: Loss 0.2370\n",
            "Batch 33400: Loss 0.3054\n",
            "Batch 33500: Loss 0.2728\n",
            "Batch 33600: Loss 0.2696\n",
            "Batch 33700: Loss 0.2612\n",
            "Batch 33800: Loss 0.2397\n",
            "Batch 33900: Loss 0.2652\n",
            "Batch 34000: Loss 0.2109\n",
            "Batch 34100: Loss 0.3263\n",
            "Batch 34200: Loss 0.2929\n",
            "Batch 34300: Loss 0.2563\n",
            "Batch 34400: Loss 0.2760\n",
            "Batch 34500: Loss 0.2066\n",
            "Batch 34600: Loss 0.2748\n",
            "Batch 34700: Loss 0.2924\n",
            "Batch 34800: Loss 0.2414\n",
            "Batch 34900: Loss 0.3000\n",
            "Batch 35000: Loss 0.2741\n",
            "Batch 35100: Loss 0.2345\n",
            "Batch 35200: Loss 0.2392\n",
            "Batch 35300: Loss 0.2587\n",
            "Batch 35400: Loss 0.2665\n",
            "Batch 35500: Loss 0.2682\n",
            "Batch 35600: Loss 0.2505\n",
            "Batch 35700: Loss 0.2170\n",
            "Batch 35800: Loss 0.2691\n",
            "Batch 35900: Loss 0.2052\n",
            "Batch 36000: Loss 0.2697\n",
            "Batch 36100: Loss 0.2290\n",
            "Batch 36200: Loss 0.2502\n",
            "Batch 36300: Loss 0.2370\n",
            "Batch 36400: Loss 0.2585\n",
            "Batch 36500: Loss 0.2938\n",
            "Batch 36600: Loss 0.1993\n",
            "Batch 36700: Loss 0.2096\n",
            "Average loss: 0.7300\n",
            "Saved new best model with loss: 0.7300\n",
            "\n",
            "Generated sample:\n",
            "s BLIND EYES catch the light, and it STOPS. As if...\n",
            "listening.\n",
            "ARAGOG\n",
            "You do not come from the forest.\n",
            "Your hearts beat like... men.\n",
            "HARRY\n",
            "You heard Hermione, and now exactly dher\n",
            "detention... For them.\n",
            "UNCLE VERNON\n",
            "I'm sorry. \n",
            "\n",
            "Epoch 2/10\n",
            "Batch 0: Loss 0.2677\n",
            "Batch 100: Loss 0.2441\n",
            "Batch 200: Loss 0.2727\n",
            "Batch 300: Loss 0.2736\n",
            "Batch 400: Loss 0.2687\n",
            "Batch 500: Loss 0.2234\n",
            "Batch 600: Loss 0.3939\n",
            "Batch 700: Loss 0.3392\n",
            "Batch 800: Loss 0.2911\n",
            "Batch 900: Loss 0.2288\n",
            "Batch 1000: Loss 0.2485\n",
            "Batch 1100: Loss 0.2402\n",
            "Batch 1200: Loss 0.2405\n",
            "Batch 1300: Loss 0.2038\n",
            "Batch 1400: Loss 0.2495\n",
            "Batch 1500: Loss 0.2156\n",
            "Batch 1600: Loss 0.2705\n",
            "Batch 1700: Loss 0.2476\n",
            "Batch 1800: Loss 0.2227\n",
            "Batch 1900: Loss 0.2035\n",
            "Batch 2000: Loss 0.2927\n",
            "Batch 2100: Loss 0.2854\n",
            "Batch 2200: Loss 0.2648\n",
            "Batch 2300: Loss 0.2890\n",
            "Batch 2400: Loss 0.2047\n",
            "Batch 2500: Loss 0.2274\n",
            "Batch 2600: Loss 0.2602\n",
            "Batch 2700: Loss 0.2541\n",
            "Batch 2800: Loss 0.2403\n",
            "Batch 2900: Loss 0.2734\n",
            "Batch 3000: Loss 0.2525\n",
            "Batch 3100: Loss 0.2012\n",
            "Batch 3200: Loss 0.2287\n",
            "Batch 3300: Loss 0.2755\n",
            "Batch 3400: Loss 0.2868\n",
            "Batch 3500: Loss 0.2026\n",
            "Batch 3600: Loss 0.2254\n",
            "Batch 3700: Loss 0.2717\n",
            "Batch 3800: Loss 0.2587\n",
            "Batch 3900: Loss 0.2487\n",
            "Batch 4000: Loss 0.2327\n",
            "Batch 4100: Loss 0.1656\n",
            "Batch 4200: Loss 0.2040\n",
            "Batch 4300: Loss 0.2399\n",
            "Batch 4400: Loss 0.1729\n",
            "Batch 4500: Loss 0.1953\n",
            "Batch 4600: Loss 0.2559\n",
            "Batch 4700: Loss 0.2572\n",
            "Batch 4800: Loss 0.2751\n",
            "Batch 4900: Loss 0.2324\n",
            "Batch 5000: Loss 0.2372\n",
            "Batch 5100: Loss 0.2653\n",
            "Batch 5200: Loss 0.2011\n",
            "Batch 5300: Loss 0.2508\n",
            "Batch 5400: Loss 0.2351\n",
            "Batch 5500: Loss 0.2732\n",
            "Batch 5600: Loss 0.2231\n",
            "Batch 5700: Loss 0.2264\n",
            "Batch 5800: Loss 0.2177\n",
            "Batch 5900: Loss 0.2215\n",
            "Batch 6000: Loss 0.2988\n",
            "Batch 6100: Loss 0.1909\n",
            "Batch 6200: Loss 0.2248\n",
            "Batch 6300: Loss 0.2456\n",
            "Batch 6400: Loss 0.2169\n",
            "Batch 6500: Loss 0.2421\n",
            "Batch 6600: Loss 0.2054\n",
            "Batch 6700: Loss 0.2144\n",
            "Batch 6800: Loss 0.2128\n",
            "Batch 6900: Loss 0.2202\n",
            "Batch 7000: Loss 0.2383\n",
            "Batch 7100: Loss 0.2748\n",
            "Batch 7200: Loss 0.1620\n",
            "Batch 7300: Loss 0.2250\n",
            "Batch 7400: Loss 0.2272\n",
            "Batch 7500: Loss 0.2343\n",
            "Batch 7600: Loss 0.1905\n",
            "Batch 7700: Loss 0.1940\n",
            "Batch 7800: Loss 0.3234\n",
            "Batch 7900: Loss 0.2578\n",
            "Batch 8000: Loss 0.1965\n",
            "Batch 8100: Loss 0.2525\n",
            "Batch 8200: Loss 0.2557\n",
            "Batch 8300: Loss 0.1928\n",
            "Batch 8400: Loss 0.2067\n",
            "Batch 8500: Loss 0.2378\n",
            "Batch 8600: Loss 0.3023\n",
            "Batch 8700: Loss 0.2492\n",
            "Batch 8800: Loss 0.1778\n",
            "Batch 8900: Loss 0.2191\n",
            "Batch 9000: Loss 0.2491\n",
            "Batch 9100: Loss 0.1830\n",
            "Batch 9200: Loss 0.1799\n",
            "Batch 9300: Loss 0.1908\n",
            "Batch 9400: Loss 0.2399\n",
            "Batch 9500: Loss 0.2112\n",
            "Batch 9600: Loss 0.2232\n",
            "Batch 9700: Loss 0.1770\n",
            "Batch 9800: Loss 0.2262\n",
            "Batch 9900: Loss 0.2130\n",
            "Batch 10000: Loss 0.2320\n",
            "Batch 10100: Loss 0.2212\n",
            "Batch 10200: Loss 0.2414\n",
            "Batch 10300: Loss 0.2610\n",
            "Batch 10400: Loss 0.1977\n",
            "Batch 10500: Loss 0.1956\n",
            "Batch 10600: Loss 0.2114\n",
            "Batch 10700: Loss 0.2234\n",
            "Batch 10800: Loss 0.2012\n",
            "Batch 10900: Loss 0.1963\n",
            "Batch 11000: Loss 0.2620\n",
            "Batch 11100: Loss 0.2371\n",
            "Batch 11200: Loss 0.2127\n",
            "Batch 11300: Loss 0.2149\n",
            "Batch 11400: Loss 0.2670\n",
            "Batch 11500: Loss 0.1924\n",
            "Batch 11600: Loss 0.2499\n",
            "Batch 11700: Loss 0.1787\n",
            "Batch 11800: Loss 0.2724\n",
            "Batch 11900: Loss 0.1900\n",
            "Batch 12000: Loss 0.2863\n",
            "Batch 12100: Loss 0.1530\n",
            "Batch 12200: Loss 0.2041\n",
            "Batch 12300: Loss 0.1635\n",
            "Batch 12400: Loss 0.2255\n",
            "Batch 12500: Loss 0.2026\n",
            "Batch 12600: Loss 0.2657\n",
            "Batch 12700: Loss 0.2289\n",
            "Batch 12800: Loss 0.2122\n",
            "Batch 12900: Loss 0.2304\n",
            "Batch 13000: Loss 0.1770\n",
            "Batch 13100: Loss 0.2130\n",
            "Batch 13200: Loss 0.2305\n",
            "Batch 13300: Loss 0.2273\n",
            "Batch 13400: Loss 0.1897\n",
            "Batch 13500: Loss 0.2139\n",
            "Batch 13600: Loss 0.2252\n",
            "Batch 13700: Loss 0.2006\n",
            "Batch 13800: Loss 0.2342\n",
            "Batch 13900: Loss 0.1712\n",
            "Batch 14000: Loss 0.2226\n",
            "Batch 14100: Loss 0.2335\n",
            "Batch 14200: Loss 0.1675\n",
            "Batch 14300: Loss 0.1911\n",
            "Batch 14400: Loss 0.2160\n",
            "Batch 14500: Loss 0.2326\n",
            "Batch 14600: Loss 0.1988\n",
            "Batch 14700: Loss 0.2701\n",
            "Batch 14800: Loss 0.2302\n",
            "Batch 14900: Loss 0.1924\n",
            "Batch 15000: Loss 0.1971\n",
            "Batch 15100: Loss 0.1976\n",
            "Batch 15200: Loss 0.2191\n",
            "Batch 15300: Loss 0.2057\n",
            "Batch 15400: Loss 0.2032\n",
            "Batch 15500: Loss 0.1969\n",
            "Batch 15600: Loss 0.2517\n",
            "Batch 15700: Loss 0.1949\n",
            "Batch 15800: Loss 0.2184\n",
            "Batch 15900: Loss 0.1915\n",
            "Batch 16000: Loss 0.1954\n",
            "Batch 16100: Loss 0.2319\n",
            "Batch 16200: Loss 0.2293\n",
            "Batch 16300: Loss 0.2337\n",
            "Batch 16400: Loss 0.2388\n",
            "Batch 16500: Loss 0.2043\n",
            "Batch 16600: Loss 0.2514\n",
            "Batch 16700: Loss 0.2254\n",
            "Batch 16800: Loss 0.2262\n",
            "Batch 16900: Loss 0.2197\n",
            "Batch 17000: Loss 0.1771\n",
            "Batch 17100: Loss 0.1671\n",
            "Batch 17200: Loss 0.2172\n",
            "Batch 17300: Loss 0.1792\n",
            "Batch 17400: Loss 0.2094\n",
            "Batch 17500: Loss 0.1636\n",
            "Batch 17600: Loss 0.1741\n",
            "Batch 17700: Loss 0.2176\n",
            "Batch 17800: Loss 0.2065\n",
            "Batch 17900: Loss 0.2463\n",
            "Batch 18000: Loss 0.2895\n",
            "Batch 18100: Loss 0.2367\n",
            "Batch 18200: Loss 0.1942\n",
            "Batch 18300: Loss 0.1907\n",
            "Batch 18400: Loss 0.2187\n",
            "Batch 18500: Loss 0.2003\n",
            "Batch 18600: Loss 0.1837\n",
            "Batch 18700: Loss 0.2137\n",
            "Batch 18800: Loss 0.2153\n",
            "Batch 18900: Loss 0.1597\n",
            "Batch 19000: Loss 0.1962\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[65], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save if best model\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[64], line 21\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, device)\u001b[0m\n\u001b[0;32m     18\u001b[0m x, y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[63], line 45\u001b[0m, in \u001b[0;36mGPT.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# Apply transformer blocks\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m---> 45\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Apply final layer norm\u001b[39;00m\n\u001b[0;32m     48\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[62], line 26\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# Self-attention with residual connection\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m# Feed-forward with residual connection\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[1;32mIn[61], line 49\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Project back to embedding dimension\u001b[39;00m\n\u001b[0;32m     48\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out)\n\u001b[1;32m---> 49\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\ASUS\\miniconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1426\u001b[0m )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training loop with text generation samples\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    \n",
        "    # Train for one epoch\n",
        "    avg_loss = train_epoch(model, dataloader, optimizer, device)\n",
        "    print(f\"Average loss: {avg_loss:.4f}\")\n",
        "    \n",
        "    # Save if best model\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(f\"Saved new best model with loss: {best_loss:.4f}\")\n",
        "    \n",
        "    # Generate sample text\n",
        "    if (epoch + 1) % 1 == 0:  # Generate every epoch\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            # Start with a random context from the dataset\n",
        "            context = next(iter(dataloader))[0][0:1].to(device)  # Take first sequence of first batch\n",
        "            \n",
        "            # Generate 100 new tokens\n",
        "            generated = model.generate(context, max_new_tokens=100, temperature=0.8, top_k=40)\n",
        "            \n",
        "            # Convert to text\n",
        "            generated_text = ''.join([itos[int(i)] for i in generated[0].tolist()])\n",
        "            print(\"\\nGenerated sample:\")\n",
        "            print(generated_text)\n",
        "            \n",
        "print(\"\\nTraining completed!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
